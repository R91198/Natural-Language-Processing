{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hi there, in this tutorial we'll see how a basic Tokenizer works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer is your friend when it comes to doing natural language processing. It does all the heavy lifting of managing tokens, turning your texts into streams of tokens, etc. The reason why we need this is that when it comes to training neural networks, you're going to be doing a lot more maths and maths deals with numbers, and instead of having the words being trained in the neural network, you can actually have the number representing the words and it just makes you're life much easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"
     ]
    }
   ],
   "source": [
    "# So here, we have these two senetences and we're going to tokenize these using the Tokenizer.\n",
    "sentences = [ 'I love my dog', \n",
    "            'I, love my cat']\n",
    "\n",
    "# Here we create the instance for the Tokenizer.\n",
    "# Here, 'num_words' parameter is going to take the most common 100 words(or whichever the value you choose).\n",
    "# The 'fit_on_texts' will then do is that it will go through the entire body of text and it will create a dictionary\n",
    "# with the key being the word and the value being the token of that word.\n",
    "tokenizer = Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few things to note in this is that the punctuations like comma and spaces have actually been removed. What that means is that it cleans up the texts as well to actually pull out the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
     ]
    }
   ],
   "source": [
    "sentences = [ 'I love my dog', \n",
    "            'i, love my cat',\n",
    "            'You love my dog!']\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if I change this a little by adding some few words to it. Notice tha 'You' is capitalised and 'dog' has an exclamation sign to it, but it's not going to confuse that with the previous 'dog'. And, when we run this we see that now we have a whole new set of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, that's a basic introduction to how the tokenizer actually works. Now, give this a try for yourself and let me know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
