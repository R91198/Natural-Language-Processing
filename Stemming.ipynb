{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before moving to the application part, let's try to understand what is 'Stemming' and why do we need it ? ü§îü§î"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, one important thing in NLP is that all the input data is in the form of text and we cannot just pass this text data to our model because the model will not be able to understand what is that text data. So, what we do is that we try to pre-process that data and we also try to convert it into some numerical representation which we basically call it as vectors. So, 'Stemming' is a process wherein we are trying to reduce the infected word to the word stem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we need 'Stemming' because in NLP, most of the problems that we see like Sentiment Analysis, Determining Spam Classifier, Understanding Movie or Restaurant Reviews and provide some ratings based on that, are determined only by the word stem words. So, just by finding out a particular word stem, we would be able to determine if the word is positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you've understood the main cause behind Stemming and why we use it so often. Now, let's dig into the application part of this tutorial. Yay...üòÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import nltk\n",
    "\n",
    "# Helps to implement Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Used for the removal of common words such as 'of', 'the', 'a',etc which are not relevant to our model\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Real Madrid simply show no sign of letting up. The LaLiga table-toppers saw off Alav√©s at \n",
    "               the Di St√©fano to make it eight wins on the bounce and retain the four-point buffer at the \n",
    "               summit with three games to go. The Madrid goals came from Karim Benzema, who converted \n",
    "               from the spot, whilst Marco Asensio was also on the mark for the hosts, who recorded a fifth \n",
    "               successive shutout. Ferland Mendy started at left wing-back, with Lucas V√°zquez occupying \n",
    "               the right wing-back berth and inside the first minute, the pair were involved in the madridistas' \n",
    "               first forward foray, which culminated in Luka Modric sending his effort wide of the target. \n",
    "               The Alav√©s response wasn't long in coming and Joselu's headed effort struck the crossbar, \n",
    "               whilst Rapha√´l Varane cleared a Lucas P√©rez's follow-up off the line. It looked as if we were \n",
    "               in store for a high-tempo affair and just after the 10-minute mark, Mendy once again showed what a \n",
    "               threat he is down the left. Ximo Navarro upended the Frenchman in the area and Benzema stepped \n",
    "               up to make it 1-0. With 12 minutes gone, Toni Kroos‚Äô did his best to find the top corner, before a \n",
    "               fierce Mendy cross nearly forced Camarasa to turn into his own net on 17‚Äô. The Blanquiazules refused\n",
    "               to roll over though, with Oliver Burke proving a constant nuisance for the defence and testing\n",
    "               Thibaut Courtois, despite the final chances before the break falling to Rodrygo and Benzema. \n",
    "               After the restart, referee Gil Manzano retired injured and by the time the 50th minute came around, \n",
    "               Madrid had added to their advantage. Benzema and Asensio raced through on goal, up against Roberto, \n",
    "               and the Balearic Island-born forward stroked home with ease, though his goal was originally ruled \n",
    "               out for offside before being correctly awarded by VAR.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Real Madrid simply show no sign of letting up.',\n",
       " 'The LaLiga table-toppers saw off Alav√©s at \\n               the Di St√©fano to make it eight wins on the bounce and retain the four-point buffer at the \\n               summit with three games to go.',\n",
       " 'The Madrid goals came from Karim Benzema, who converted \\n               from the spot, whilst Marco Asensio was also on the mark for the hosts, who recorded a fifth \\n               successive shutout.',\n",
       " \"Ferland Mendy started at left wing-back, with Lucas V√°zquez occupying \\n               the right wing-back berth and inside the first minute, the pair were involved in the madridistas' \\n               first forward foray, which culminated in Luka Modric sending his effort wide of the target.\",\n",
       " \"The Alav√©s response wasn't long in coming and Joselu's headed effort struck the crossbar, \\n               whilst Rapha√´l Varane cleared a Lucas P√©rez's follow-up off the line.\",\n",
       " 'It looked as if we were \\n               in store for a high-tempo affair and just after the 10-minute mark, Mendy once again showed what a \\n               threat he is down the left.',\n",
       " 'Ximo Navarro upended the Frenchman in the area and Benzema stepped \\n               up to make it 1-0.',\n",
       " 'With 12 minutes gone, Toni Kroos‚Äô did his best to find the top corner, before a \\n               fierce Mendy cross nearly forced Camarasa to turn into his own net on 17‚Äô.',\n",
       " 'The Blanquiazules refused\\n               to roll over though, with Oliver Burke proving a constant nuisance for the defence and testing\\n               Thibaut Courtois, despite the final chances before the break falling to Rodrygo and Benzema.',\n",
       " 'After the restart, referee Gil Manzano retired injured and by the time the 50th minute came around, \\n               Madrid had added to their advantage.',\n",
       " 'Benzema and Asensio raced through on goal, up against Roberto, \\n               and the Balearic Island-born forward stroked home with ease, though his goal was originally ruled \\n               out for offside before being correctly awarded by VAR.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the whole paragraph into sentences\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of words that we don't want. Also, we can change this language parameter to whichever language you want\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de',\n",
       " 'la',\n",
       " 'que',\n",
       " 'el',\n",
       " 'en',\n",
       " 'y',\n",
       " 'a',\n",
       " 'los',\n",
       " 'del',\n",
       " 'se',\n",
       " 'las',\n",
       " 'por',\n",
       " 'un',\n",
       " 'para',\n",
       " 'con',\n",
       " 'no',\n",
       " 'una',\n",
       " 'su',\n",
       " 'al',\n",
       " 'lo',\n",
       " 'como',\n",
       " 'm√°s',\n",
       " 'pero',\n",
       " 'sus',\n",
       " 'le',\n",
       " 'ya',\n",
       " 'o',\n",
       " 'este',\n",
       " 's√≠',\n",
       " 'porque',\n",
       " 'esta',\n",
       " 'entre',\n",
       " 'cuando',\n",
       " 'muy',\n",
       " 'sin',\n",
       " 'sobre',\n",
       " 'tambi√©n',\n",
       " 'me',\n",
       " 'hasta',\n",
       " 'hay',\n",
       " 'donde',\n",
       " 'quien',\n",
       " 'desde',\n",
       " 'todo',\n",
       " 'nos',\n",
       " 'durante',\n",
       " 'todos',\n",
       " 'uno',\n",
       " 'les',\n",
       " 'ni',\n",
       " 'contra',\n",
       " 'otros',\n",
       " 'ese',\n",
       " 'eso',\n",
       " 'ante',\n",
       " 'ellos',\n",
       " 'e',\n",
       " 'esto',\n",
       " 'm√≠',\n",
       " 'antes',\n",
       " 'algunos',\n",
       " 'qu√©',\n",
       " 'unos',\n",
       " 'yo',\n",
       " 'otro',\n",
       " 'otras',\n",
       " 'otra',\n",
       " '√©l',\n",
       " 'tanto',\n",
       " 'esa',\n",
       " 'estos',\n",
       " 'mucho',\n",
       " 'quienes',\n",
       " 'nada',\n",
       " 'muchos',\n",
       " 'cual',\n",
       " 'poco',\n",
       " 'ella',\n",
       " 'estar',\n",
       " 'estas',\n",
       " 'algunas',\n",
       " 'algo',\n",
       " 'nosotros',\n",
       " 'mi',\n",
       " 'mis',\n",
       " 't√∫',\n",
       " 'te',\n",
       " 'ti',\n",
       " 'tu',\n",
       " 'tus',\n",
       " 'ellas',\n",
       " 'nosotras',\n",
       " 'vosotros',\n",
       " 'vosotras',\n",
       " 'os',\n",
       " 'm√≠o',\n",
       " 'm√≠a',\n",
       " 'm√≠os',\n",
       " 'm√≠as',\n",
       " 'tuyo',\n",
       " 'tuya',\n",
       " 'tuyos',\n",
       " 'tuyas',\n",
       " 'suyo',\n",
       " 'suya',\n",
       " 'suyos',\n",
       " 'suyas',\n",
       " 'nuestro',\n",
       " 'nuestra',\n",
       " 'nuestros',\n",
       " 'nuestras',\n",
       " 'vuestro',\n",
       " 'vuestra',\n",
       " 'vuestros',\n",
       " 'vuestras',\n",
       " 'esos',\n",
       " 'esas',\n",
       " 'estoy',\n",
       " 'est√°s',\n",
       " 'est√°',\n",
       " 'estamos',\n",
       " 'est√°is',\n",
       " 'est√°n',\n",
       " 'est√©',\n",
       " 'est√©s',\n",
       " 'estemos',\n",
       " 'est√©is',\n",
       " 'est√©n',\n",
       " 'estar√©',\n",
       " 'estar√°s',\n",
       " 'estar√°',\n",
       " 'estaremos',\n",
       " 'estar√©is',\n",
       " 'estar√°n',\n",
       " 'estar√≠a',\n",
       " 'estar√≠as',\n",
       " 'estar√≠amos',\n",
       " 'estar√≠ais',\n",
       " 'estar√≠an',\n",
       " 'estaba',\n",
       " 'estabas',\n",
       " 'est√°bamos',\n",
       " 'estabais',\n",
       " 'estaban',\n",
       " 'estuve',\n",
       " 'estuviste',\n",
       " 'estuvo',\n",
       " 'estuvimos',\n",
       " 'estuvisteis',\n",
       " 'estuvieron',\n",
       " 'estuviera',\n",
       " 'estuvieras',\n",
       " 'estuvi√©ramos',\n",
       " 'estuvierais',\n",
       " 'estuvieran',\n",
       " 'estuviese',\n",
       " 'estuvieses',\n",
       " 'estuvi√©semos',\n",
       " 'estuvieseis',\n",
       " 'estuviesen',\n",
       " 'estando',\n",
       " 'estado',\n",
       " 'estada',\n",
       " 'estados',\n",
       " 'estadas',\n",
       " 'estad',\n",
       " 'he',\n",
       " 'has',\n",
       " 'ha',\n",
       " 'hemos',\n",
       " 'hab√©is',\n",
       " 'han',\n",
       " 'haya',\n",
       " 'hayas',\n",
       " 'hayamos',\n",
       " 'hay√°is',\n",
       " 'hayan',\n",
       " 'habr√©',\n",
       " 'habr√°s',\n",
       " 'habr√°',\n",
       " 'habremos',\n",
       " 'habr√©is',\n",
       " 'habr√°n',\n",
       " 'habr√≠a',\n",
       " 'habr√≠as',\n",
       " 'habr√≠amos',\n",
       " 'habr√≠ais',\n",
       " 'habr√≠an',\n",
       " 'hab√≠a',\n",
       " 'hab√≠as',\n",
       " 'hab√≠amos',\n",
       " 'hab√≠ais',\n",
       " 'hab√≠an',\n",
       " 'hube',\n",
       " 'hubiste',\n",
       " 'hubo',\n",
       " 'hubimos',\n",
       " 'hubisteis',\n",
       " 'hubieron',\n",
       " 'hubiera',\n",
       " 'hubieras',\n",
       " 'hubi√©ramos',\n",
       " 'hubierais',\n",
       " 'hubieran',\n",
       " 'hubiese',\n",
       " 'hubieses',\n",
       " 'hubi√©semos',\n",
       " 'hubieseis',\n",
       " 'hubiesen',\n",
       " 'habiendo',\n",
       " 'habido',\n",
       " 'habida',\n",
       " 'habidos',\n",
       " 'habidas',\n",
       " 'soy',\n",
       " 'eres',\n",
       " 'es',\n",
       " 'somos',\n",
       " 'sois',\n",
       " 'son',\n",
       " 'sea',\n",
       " 'seas',\n",
       " 'seamos',\n",
       " 'se√°is',\n",
       " 'sean',\n",
       " 'ser√©',\n",
       " 'ser√°s',\n",
       " 'ser√°',\n",
       " 'seremos',\n",
       " 'ser√©is',\n",
       " 'ser√°n',\n",
       " 'ser√≠a',\n",
       " 'ser√≠as',\n",
       " 'ser√≠amos',\n",
       " 'ser√≠ais',\n",
       " 'ser√≠an',\n",
       " 'era',\n",
       " 'eras',\n",
       " '√©ramos',\n",
       " 'erais',\n",
       " 'eran',\n",
       " 'fui',\n",
       " 'fuiste',\n",
       " 'fue',\n",
       " 'fuimos',\n",
       " 'fuisteis',\n",
       " 'fueron',\n",
       " 'fuera',\n",
       " 'fueras',\n",
       " 'fu√©ramos',\n",
       " 'fuerais',\n",
       " 'fueran',\n",
       " 'fuese',\n",
       " 'fueses',\n",
       " 'fu√©semos',\n",
       " 'fueseis',\n",
       " 'fuesen',\n",
       " 'sintiendo',\n",
       " 'sentido',\n",
       " 'sentida',\n",
       " 'sentidos',\n",
       " 'sentidas',\n",
       " 'siente',\n",
       " 'sentid',\n",
       " 'tengo',\n",
       " 'tienes',\n",
       " 'tiene',\n",
       " 'tenemos',\n",
       " 'ten√©is',\n",
       " 'tienen',\n",
       " 'tenga',\n",
       " 'tengas',\n",
       " 'tengamos',\n",
       " 'teng√°is',\n",
       " 'tengan',\n",
       " 'tendr√©',\n",
       " 'tendr√°s',\n",
       " 'tendr√°',\n",
       " 'tendremos',\n",
       " 'tendr√©is',\n",
       " 'tendr√°n',\n",
       " 'tendr√≠a',\n",
       " 'tendr√≠as',\n",
       " 'tendr√≠amos',\n",
       " 'tendr√≠ais',\n",
       " 'tendr√≠an',\n",
       " 'ten√≠a',\n",
       " 'ten√≠as',\n",
       " 'ten√≠amos',\n",
       " 'ten√≠ais',\n",
       " 'ten√≠an',\n",
       " 'tuve',\n",
       " 'tuviste',\n",
       " 'tuvo',\n",
       " 'tuvimos',\n",
       " 'tuvisteis',\n",
       " 'tuvieron',\n",
       " 'tuviera',\n",
       " 'tuvieras',\n",
       " 'tuvi√©ramos',\n",
       " 'tuvierais',\n",
       " 'tuvieran',\n",
       " 'tuviese',\n",
       " 'tuvieses',\n",
       " 'tuvi√©semos',\n",
       " 'tuvieseis',\n",
       " 'tuviesen',\n",
       " 'teniendo',\n",
       " 'tenido',\n",
       " 'tenida',\n",
       " 'tenidos',\n",
       " 'tenidas',\n",
       " 'tened']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same stopwords in spanish!\n",
    "stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now what we're trying to do is that first, we need to remove the stopwords from each and every sentence that we have in our data. And then, after removing the stopwords we would then apply stemming to each word present in each of the sentences in the paragraph.So, let's see how we can do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stemming\n",
    "# If the word belongs to the stopwords then we're gonna remove it. Otherwise, we'll apply stemming to that\n",
    "# And then finally, we're gonna join the combination of words that have been stemmed\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['benzema',\n",
       " 'asensio',\n",
       " 'race',\n",
       " 'goal',\n",
       " ',',\n",
       " 'roberto',\n",
       " ',',\n",
       " 'balear',\n",
       " 'island-born',\n",
       " 'forward',\n",
       " 'stroke',\n",
       " 'home',\n",
       " 'eas',\n",
       " ',',\n",
       " 'though',\n",
       " 'goal',\n",
       " 'origin',\n",
       " 'rule',\n",
       " 'offsid',\n",
       " 'correctli',\n",
       " 'award',\n",
       " 'var',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resultant list of words\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'benzema asensio race goal , roberto , balear island-born forward stroke home eas , though goal origin rule offsid correctli award var .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resultant sentences\n",
    "sentences[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The main problem regarding 'Stemming' is that it produces intermediate representation of the word i.e some of the words in the corpus doesn't have any meaning to the human language. So for this reason, we use Lemmatization in many cases and what that does is that it not only stems the sentences but also produces meaningful words in it's corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, it's your turn to try this out by yourself. Till then, PEACE...‚úåÔ∏è "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
